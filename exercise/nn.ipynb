{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a simple NN from scratch with numPy\n",
    "\n",
    "Implement the missing functionalities one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from dreader import MnistDataloader, show_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import (Given)\n",
    "\n",
    "We use the MNIST dataset, it contains 28x28 images of handwritten digits, we want to classify. The training data contains 60k images, the test dataset contains 10k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/mnist/\"\n",
    "\n",
    "# paths\n",
    "training_images_filepath = join(\n",
    "    DATA_PATH, \"train-images-idx3-ubyte/train-images-idx3-ubyte\"\n",
    ")\n",
    "training_labels_filepath = join(\n",
    "    DATA_PATH, \"train-labels-idx1-ubyte/train-labels-idx1-ubyte\"\n",
    ")\n",
    "test_images_filepath = join(DATA_PATH, \"t10k-images-idx3-ubyte/t10k-images-idx3-ubyte\")\n",
    "test_labels_filepath = join(DATA_PATH, \"t10k-labels-idx1-ubyte/t10k-labels-idx1-ubyte\")\n",
    "\n",
    "mnist_dataloader = MnistDataloader(\n",
    "    training_images_filepath,\n",
    "    training_labels_filepath,\n",
    "    test_images_filepath,\n",
    "    test_labels_filepath,\n",
    ")\n",
    "(x_train, y_train), (x_test, y_test) = mnist_dataloader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_2_show = []\n",
    "titles_2_show = []\n",
    "for i in range(0, 10):\n",
    "    r = random.randint(1, 60000)\n",
    "    images_2_show.append(x_train[r])\n",
    "    titles_2_show.append(\"training image [\" + str(r) + \"] = \" + str(y_train[r]))\n",
    "\n",
    "for i in range(0, 5):\n",
    "    r = random.randint(1, 10000)\n",
    "    images_2_show.append(x_test[r])\n",
    "    titles_2_show.append(\"test image [\" + str(r) + \"] = \" + str(y_test[r]))\n",
    "\n",
    "show_images(images_2_show, titles_2_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data for Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD: convert data into numpy arrays and normalize values between to 0-1 (currently its 0-255) (x = image array, y = label)\n",
    "TRAINING_SIZE = 60000  # max 60000\n",
    "\n",
    "x_train = \n",
    "y_train = \n",
    "x_test = \n",
    "y_test = \n",
    "\n",
    "# transform the data into 2D arrays with 28*28=784 pixels\n",
    "input_size = x_train.shape[1] * x_train.shape[2]\n",
    "x_train = x_train.reshape(x_train.shape[0], input_size)\n",
    "x_test = x_test.reshape(x_test.shape[0], input_size)\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Network\n",
    "\n",
    "Complete the function init_params to return the arrays, defining the tunable parameters (weights W, biases b per layer).\n",
    "We will build a network with two trainable layers of size 10 each. What is the size of the input layer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    W1 = \n",
    "    b1 = \n",
    "    W2 = \n",
    "    b2 = \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Activation Functions\n",
    "\n",
    "The activation functions are crucial for learning more complex patterns as they break up the currently linear flow by introducing non-linearity.\n",
    "\n",
    "Set up a relu function (takes an np.array as an input and outputs an np.array where each val is the output of the relu).\n",
    "Set up a softmax function for the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "\n",
    "Now we want to implement the forward pass through the network. Simply calculating the next layers values by Wx + b first, and adding the activation function next. Output the vals before each activation function (Z1, Z2) and after the relu and the softmax (A1, A2).\n",
    "\n",
    "Using the dot product of the weights with the previous input x and adding the bias b computes the value all at once, before the activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W1, b1, W2, b2, x):\n",
    "    Z1 = \n",
    "    A1 = \n",
    "    Z2 = \n",
    "    A2 = \n",
    "\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the error by Backpropagation\n",
    "\n",
    "This is the core function for learning. After the forward pass, we compare the difference between the expected result in one hot (e.g. 0 1 0... for label 2) and the output layer. This gives us dZ2, the error in layer 2 (the output layer). Calculate dW2, db2, dZ1, dW1 and db1 accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper\n",
    "def one_hot(y):\n",
    "    one_hot_y = np.zeros(\n",
    "        (y.size, y.max() + 1)\n",
    "    )  # create a matrix of y.size rows and y.max()+1 (10) columns\n",
    "    one_hot_y[np.arange(y.size), y] = (\n",
    "        1  # set the value of the column at the index of y to 1\n",
    "    )\n",
    "    one_hot_y = (\n",
    "        one_hot_y.T\n",
    "    )  # transpose the matrix bc we want each column to be an example\n",
    "\n",
    "    return one_hot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deriv_relu(x):\n",
    "    pass\n",
    "\n",
    "\n",
    "def back_prop(Z1, A1, Z2, A2, W1, W2, x, y):\n",
    "    dZ2 = \n",
    "    dW2 = \n",
    "    db2 = \n",
    "    dZ1 = \n",
    "    dW1 = \n",
    "    db1 = \n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the parameters by the errors calculated through Backprop\n",
    "\n",
    "Nudge each value by the calculated error (d..) times the learning rate and subtract this from the previous value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
    "    W1 = \n",
    "    b1 = \n",
    "    W2 = \n",
    "    b2 = \n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy and loss function\n",
    "\n",
    "Used to get insights into training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A2):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_accuracy(A2, y):\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_loss(A2, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement learning algorithm (gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x, y, iters, alpha):\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "    for i in range(iters):\n",
    "        pass\n",
    "\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(x_train, y_train, 1000, 0.08)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, x_test.T)\n",
    "predictions = get_predictions(A2)\n",
    "\n",
    "print(\"Test accuracy: \", get_accuracy(A2, y_test))\n",
    "print(\"Test loss: \", get_loss(A2, y_test))\n",
    "print(\"Predictions: \", predictions)\n",
    "print(\"True values: \", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "classes = np.unique(y_test)\n",
    "accuracy_per_class = {}\n",
    "\n",
    "for cls in classes:\n",
    "    idx = (y_test == cls)\n",
    "    cls_accuracy = np.mean(predictions[idx] == y_test[idx])\n",
    "    accuracy_per_class[cls] = cls_accuracy\n",
    "\n",
    "# Plotting accuracy per digit\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.bar(accuracy_per_class.keys(), accuracy_per_class.values(), color='skyblue')\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Digit')\n",
    "plt.xticks(classes)\n",
    "plt.ylim([0, 1])\n",
    "plt.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE = True\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def save_model(W1, b1, W2, b2, path):\n",
    "    np.savez(path, W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(path):\n",
    "    data = np.load(path)\n",
    "    W1 = data['W1']\n",
    "    b1 = data['b1']\n",
    "    W2 = data['W2']\n",
    "    b2 = data['b2']\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "# Specify the path where you want to save the model\n",
    "MODEL_PATH = DATA_PATH + 'model_parameters.npz' \n",
    "\n",
    "if STORE:\n",
    "    # Save the model parameters\n",
    "    save_model(W1, b1, W2, b2, MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1_loaded, b1_loaded, W2_loaded, b2_loaded = load_model(MODEL_PATH)\n",
    "\n",
    "# Verify that the loaded parameters are the same as the original\n",
    "assert np.array_equal(W1, W1_loaded), \"W1 does not match!\"\n",
    "assert np.array_equal(b1, b1_loaded), \"b1 does not match!\"\n",
    "assert np.array_equal(W2, W2_loaded), \"W2 does not match!\"\n",
    "assert np.array_equal(b2, b2_loaded), \"b2 does not match!\"\n",
    "\n",
    "# Use the loaded model to perform forward propagation on test data\n",
    "Z1, A1, Z2, A2 = forward_prop(W1_loaded, b1_loaded, W2_loaded, b2_loaded, x_test.T)\n",
    "print(\"Test accuracy after loading model:\", get_accuracy(A2, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
